<html><head><title>The Magnitude of His Own Folly</title></head><body><h1>The Magnitude of His Own Folly</h1><p><i>Eliezer Yudkowsky, 30 September 2008 11:31AM</i></p><div><p><strong>Followup to</strong>:  <a href="0491.html">My Naturalistic Awakening</a> [http://lesswrong.com/lw/u9/my_naturalistic_awakening/], <a href="0494.html">Above-Average AI Scientists</a> [http://lesswrong.com/lw/uc/aboveaverage_ai_scientists/]</p> <p>In the years before I met that <a href="0494.html">would-be creator of Artificial General Intelligence (with a funded project) who happened to be a creationist</a> [http://lesswrong.com/lw/uc/aboveaverage_ai_scientists/], I would still try to argue with individual AGI wannabes.</p> <p>In those days, I sort-of-succeeded in convincing one such fellow that, yes, you had to take Friendly AI into account, and no, you couldn't just find the right fitness metric for an evolutionary algorithm.  (Previously he had been very impressed with evolutionary algorithms.)</p> <p>And the one said:  <em>Oh, woe!  Oh, alas!  What a fool I've been!  Through my carelessness, I almost destroyed the world!  What a villain I once was!</em></p> <p>Now, <em>there's</em> a trap I knew I better than to fall into&#8212;</p> <p>&#8212;at the point where, in late 2002, I looked back to Eliezer<sub>1997</sub>'s AI proposals and realized what they really would have done, insofar as they were coherent enough to talk about what they "really would have done".</p> <p>When I finally saw the magnitude of my own folly, everything fell into place at once.  The dam against realization cracked; and the unspoken doubts that had been accumulating behind it, crashed through all together.  There wasn't a prolonged period, or even a single moment that I remember, of wondering how I could have been so stupid.  I already knew how.</p> <p>And I also knew, all at once, in the same moment of realization, that to say, <em>I almost destroyed the world!,</em> would have been too prideful.</p> <p>It would have been too confirming of ego, too confirming of my own importance in the scheme of things, at a time when&#8212;I understood in the same moment of realization&#8212;my ego ought to be taking a major punch to the stomach.  I had been so much less than I needed to be; I had to take that punch in the stomach, not avert it.</p> <p><a id="more"></a></p> <p>And by the same token, I didn't fall into the conjugate trap of saying:  <em>Oh, well, it's not as if I had code and was about to run it; I didn't </em>really<em> come close to destroying the world.  </em>For that, too, would have minimized the force of the punch.  <em>It wasn't really loaded?</em>  I had proposed and intended to build the gun, and load the gun, and put the gun to my head and pull the trigger; and that was a bit too much <a href="0029.html">self-destructiveness</a> [http://lesswrong.com/lw/hf/debiasing_as_nonselfdestruction/].</p> <p>I didn't make a grand emotional drama out of it.  That would have wasted the force of the punch, averted it into mere tears.</p> <p>I knew, in the same moment, what I had been carefully not-doing for the last six years.  I hadn't been updating.</p> <p>And I knew I had to finally update.  To actually <em>change</em> what I planned to do, to change what I was doing now, to do something different instead.</p> <p>I knew I had to stop.</p> <p>Halt, melt, and catch fire.</p> <p>Say, "I'm not ready."  Say, "I don't know how to do this yet."</p> <p>These are terribly difficult words to say, in the field of AGI.  Both the lay audience and your fellow AGI researchers are interested in code, projects with programmers in play.  Failing that, they may give you some credit for saying, "I'm ready to write code, just give me the funding."</p> <p>Say, "I'm not ready to write code," and your status drops like a depleted uranium balloon.</p> <p>What distinguishes you, then, from six billion other people who don't know how to create Artificial General Intelligence?  If you don't have neat code (that does something other than be humanly intelligent, obviously; but at least it's code), or at minimum your own startup that's going to write code as soon as it gets funding&#8212;then who are you and what are you doing at our conference?</p> <p>Maybe later I'll post on where this attitude comes from&#8212;the excluded middle between "I know how to build AGI!" and "I'm working on narrow AI because I don't know how to build AGI", the nonexistence of a concept for "I am trying to get from an incomplete map of FAI to a complete map of FAI".</p> <p>But this attitude does exist, and so the loss of status associated with saying "I'm not ready to write code" is very great.  (If the one doubts this, let them name any other who simultaneously says "I intend to build an Artificial General Intelligence", "Right now I can't build an AGI because I don't know X", and "I am currently trying to figure out X".)</p> <p>(And never mind AGIfolk who've already raised venture capital, promising returns in five years.) </p> <p>So there's a huge reluctance to say "Stop".  You can't just say, "Oh, I'll swap back to figure-out-X mode" because that mode doesn't exist.</p> <p>Was there more to that reluctance than just loss of status, in my case?  Eliezer<sub>2001</sub> might also have flinched away from slowing his perceived forward momentum into the Singularity, which was so right and so necessary...</p> <p>But mostly, I think I flinched away from not being able to say, "I'm ready to start coding."  Not just for fear of others' reactions, but because I'd been inculcated with the same attitude myself.</p> <p>Above all, Eliezer<sub>2001</sub> didn't say "Stop"&#8212;even <em>after</em> noticing the problem of Friendly AI&#8212;because I did not realize, on a gut level, that Nature was allowed to kill me.</p> <p>"Teenagers think they're immortal", the proverb goes.  Obviously this isn't true in the literal sense that if you ask them, "Are you indestructible?" they will reply "Yes, go ahead and try shooting me."  But perhaps wearing seat belts isn't deeply emotionally compelling for them, because the thought of their own death isn't quite <em>real</em>&#8212;they don't really believe it's allowed to happen.  It can happen in <em>principle</em> but it can't <em>actually</em> happen.</p> <p>Personally, I always wore my seat belt.  As an individual, I understood that I could die.</p> <p>But, having been <a href="0482.html">raised in technophilia</a> [http://lesswrong.com/lw/u0/raised_in_technophilia/] to treasure that one most precious thing, far more important than my own life, I once thought that the Future was indestructible.</p> <p>Even when I acknowledged that nanotech could wipe out humanity, I still believed the Singularity was invulnerable.  That if humanity survived, the Singularity would happen, and it would be too smart to be corrupted or lost.</p> <p>Even after <em>that,</em> when I acknowledged Friendly AI as a consideration, I didn't emotionally believe in the possibility of failure, any more than that teenager who doesn't wear their seat belt <em>really</em> believes that an automobile accident is <em>really</em> allowed to kill or cripple them.</p> <p>It wasn't until my <a href="0491.html">insight into optimization</a> [http://lesswrong.com/lw/u9/my_naturalistic_awakening/] let me look back and see Eliezer<sub>1997</sub> in plain light, that I realized that Nature was allowed to kill me.</p> <p>"The thought you cannot think controls you more than thoughts you speak aloud."  But we flinch away from only those fears that are real to us.</p> <p>AGI researchers take very seriously the prospect of <em>someone else solving the problem first</em>.  They can imagine seeing the headlines in the paper saying that their own work has been upstaged.  They know that Nature is allowed to do that to them.  The ones who have started companies know that they are allowed to run out of venture capital.  That possibility is <em>real</em> to them, very real; it has a power of emotional compulsion over them.</p> <p>I don't think that "Oops" followed by the thud of six billion bodies falling, <em>at their own hands,</em> is real to them on quite the same level.</p> <p>It is unsafe to say what other people are thinking.  But it seems rather likely that when the one reacts to the prospect of Friendly AI by saying, "If you delay development to work on safety, other projects that don't care <em>at all</em> about Friendly AI will beat you to the punch," the prospect of they themselves making a mistake followed by six billion thuds, is not really real to them; but the possibility of others beating them to the punch is deeply scary.</p> <p>I, too, used to say things like that, before I understood that Nature was allowed to kill me.</p> <p>In that moment of realization, my childhood technophilia finally broke.</p> <p>I finally understood that even if you <a href="0353.html">diligently followed the rules of science</a> [http://lesswrong.com/lw/qf/no_safe_defense_not_even_science/] and were a nice person, Nature could still kill you.  I finally understood that even if you were the best project out of all available candidates, Nature could still kill you.</p> <p>I understood that I was not being graded on a curve.  My gaze shook free of rivals, and I saw the sheer blank wall.</p> <p>I looked back and I saw the careful arguments I had constructed, for why the wisest choice was to continue forward at full speed, just as I had planned to do before.  And I understood then that even if you constructed an argument showing that something was the best course of action, Nature was still allowed to say "So what?" and kill you.</p> <p>I looked back and saw that I had claimed to take into account the risk of a fundamental mistake, that I had argued reasons to tolerate the risk of proceeding in the absence of full knowledge.</p> <p>And I saw that the risk I wanted to tolerate would have killed me.  And I saw that this possibility had never been <em>really</em> real to me.  And I saw that even if you had wise and excellent arguments for taking a risk, the risk was still allowed to go ahead and kill you.  <em>Actually</em> kill you.</p> <p>For it is only the action that matters, and not the reasons for doing anything.  If you build the gun and load the gun and put the gun to your head and pull the trigger, even with the cleverest of arguments for carrying out every step&#8212;then, bang.</p> <p>I saw that only my own ignorance of the rules had enabled me to argue for going ahead without complete knowledge of the rules; for if you do not know the rules, you cannot model the penalty of ignorance.</p> <p>I saw that others, still ignorant of the rules, were saying "I will go ahead and do X"; and that to the extent that X was a coherent proposal at all, I knew that would result in a bang; but they said, "I do not know it cannot work".   I would try to explain to them the smallness of the target in the search space, and they would say "How can you be so sure I won't win the lottery?", wielding their own ignorance as a bludgeon.</p> <p>And so I realized that the only thing I <em>could</em> have done to save myself, in my previous state of ignorance, was to say:  "I will not proceed until I know positively that the ground is safe."  And there are many clever arguments for why you should step on a piece of ground that you don't know to contain a landmine; but they all sound much less clever, after you look to the place that you proposed and intended to step, and see the bang.</p> <p>I understood that you could do <em>everything that you were supposed to do</em>, and Nature was still allowed to kill you.  That was when my last <a href="0353.html">trust</a> [http://lesswrong.com/lw/qf/no_safe_defense_not_even_science/] broke.  And that was when my training as a rationalist began.</p> <p> </p> <p style="text-align:right">Part of the sequence <a href="http://wiki.lesswrong.com/wiki/Yudkowsky%27s_coming_of_age"><em>Yudkowsky's Coming of Age</em></a> [http://wiki.lesswrong.com/wiki/Yudkowsky%27s_coming_of_age]</p> <p style="text-align:right">Next post: "<a href="0502.html">Beyond the Reach of God</a> [http://lesswrong.com/lw/uk/beyond_the_reach_of_god/]"</p> <p style="text-align:right">Previous post: "<a href="0494.html">Above-Average AI Scientists</a> [http://lesswrong.com/lw/uc/aboveaverage_ai_scientists/]"</p></div> <hr><table><tr><th colspan="2"><a href="seq22.html">Sequence 22: Yudkowsky's Coming of Age</a>:</th></tr><tr><td><p><i>Previous: </i><a href="0494.html">Above-Average AI Scientists</a></p></td><td><p><i>Next: </i><a href="0502.html">Beyond the Reach of God</a></p></td></tr></table><p><i>Referenced by: </i><a href="0494.html">Above-Average AI Scientists</a> &#8226; <a href="0502.html">Beyond the Reach of God</a> &#8226; <a href="0503.html">My Bayesian Enlightenment</a> &#8226; <a href="0517.html">Protected From Myself</a> &#8226; <a href="0583.html">Not Taking Over the World</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/ue/the_magnitude_of_his_own_folly/">The Magnitude of His Own Folly</a></p></body></html>