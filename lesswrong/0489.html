<html><head><title>That Tiny Note of Discord</title></head><body style="width: 600px; margin: 0 auto; font-family: Georgia, serif;"><h1>That Tiny Note of Discord</h1><p><i>Eliezer Yudkowsky, 23 September 2008 06:02AM</i></p><div><p><strong>Followup to</strong>:  <a href="0484.html">The Sheer Folly of Callow Youth</a> [http://lesswrong.com/lw/u2/the_sheer_folly_of_callow_youth/]</p> <p>When we last left Eliezer<sub>1997</sub>, he believed that any superintelligence would automatically do what was "right", and indeed would understand that better than we could; even though, he modestly confessed, he did not understand the ultimate nature of morality.  Or rather, after some debate had passed, Eliezer<sub>1997</sub> had evolved an elaborate argument, which he fondly claimed to be "formal", that we could always condition upon the belief that life has meaning; and so cases where superintelligences did not feel compelled to do anything in particular, would fall out of consideration.  (The flaw being the unconsidered and unjustified equation of "universally compelling argument" with "right".)</p> <p>So far, the young Eliezer is well on the way toward joining the "smart people who are stupid because they're skilled at defending beliefs they arrived at for unskilled reasons".  All his dedication to "rationality" has not saved him from this mistake, and you might be tempted to conclude that it is useless to strive for rationality.</p> <p>But while many people dig holes for themselves, not everyone succeeds in clawing their way back out.</p> <p>And from this I learn my lesson:  That it all began&#8212;</p> <p>&#8212;with a small, small question; a single discordant note; one tiny lonely thought...</p> <p><a id="more"></a></p> <p>As our story starts, we advance three years to Eliezer<sub>2000</sub>, who in most respects resembles his self of 1997.  He currently thinks he's proven that building a superintelligence is the right thing to do if there is any right thing at all.  From which it follows that there is no <em>justifiable</em> conflict of interest over the Singularity, among the peoples and persons of Earth.</p> <p>This is an important conclusion for Eliezer<sub>2000</sub>, because he finds the notion of fighting over the Singularity to be <em>unbearably</em> stupid.  (Sort of like the notion of God intervening in fights between tribes of bickering barbarians, only in reverse.)  Eliezer<sub>2000</sub>'s self-concept does not permit him&#8212;he doesn't even <em>want</em>&#8212;to shrug and say, "Well, my side got here first, so we're going to <a href="0404.html">seize the banana</a> [http://lesswrong.com/lw/ru/the_bedrock_of_fairness/] before anyone else gets it."  It's a thought too painful to think.</p> <p>And yet then the notion occurs to him:</p> <blockquote> <p>Maybe some people would prefer an AI do particular things, such as not kill them, even if life is meaningless?</p> </blockquote> <p>His immediately following thought is the obvious one, given his premises:</p> <blockquote> <p>In the event that life is meaningless, nothing is the "right" thing to do; therefore it wouldn't be particularly right to respect people's preferences in this event.</p> </blockquote> <p>This is the obvious dodge.  The thing is, though, Eliezer<sub>2000</sub> doesn't think of himself as a villain.  He doesn't go around saying, "What bullets shall I dodge today?"  He thinks of himself as a dutiful rationalist who tenaciously follows lines of inquiry.  Later, he's going to look back and see a whole lot of inquiries that his mind somehow managed to not follow&#8212;but that's not his <em>current self-concept</em><span style="font-style: italic;">.</span><em> </em></p> <p>So Eliezer<sub>2000</sub> <em>doesn't</em> just grab the obvious out.  He keeps thinking.</p> <blockquote> <p>But if people believe they have preferences in the event that life is meaningless, then they have a motive to dispute my Singularity project and go with a project that respects their wish in the event life is meaningless.  This creates a present conflict of interest over the Singularity, and prevents right things from getting done in the mainline event that life is meaningful.</p> </blockquote> <p>Now, there's a <em>lot</em> of excuses Eliezer<sub>2000</sub> could have potentially used to toss this problem out the window.  I know, because I've <em>heard</em> plenty of excuses for dismissing Friendly AI.  "The problem is too hard to solve" is one I get from AGI wannabes who imagine themselves smart enough to create true Artificial Intelligence, but not smart enough to solve a really difficult problem like Friendly AI.  Or "worrying about this possibility would be a poor use of resources, what with the incredible urgency of creating AI before humanity wipes itself out&#8212;you've got to go with what you have", this being uttered by people who just basically aren't interested in the problem.</p> <p>But Eliezer<sub>2000</sub> is a <em>perfectionist.</em>  He's not perfect, obviously, and he doesn't attach as much importance as I do to the virtue of <em>precision,</em> but he is most certainly a <em>perfectionist.</em> The idea of metaethics that Eliezer<sub>2000</sub>  espouses, in which superintelligences know what's right better than we do, previously seemed to wrap up <em>all</em> the problems of justice and morality in an airtight wrapper.</p> <p>The new objection seems to poke a minor hole in the airtight wrapper.  This is worth patching.  If you have something that's perfect, are you really going to let one little possibility compromise it?</p> <p>So Eliezer<sub>2000</sub> doesn't even <em>want</em> to drop the issue; he wants to patch the problem and restore perfection.  How can he justify spending the time?  By thinking thoughts like:</p> <blockquote> <p>What about Brian Atkins?  [Brian Atkins being the startup funder of the Singularity Institute.]  He would probably prefer not to die, even if life were meaningless.  He's paying for the Singularity Institute right now; I don't want to taint the ethics of our cooperation.</p> </blockquote> <p>Eliezer<sub>2000</sub>'s sentiment doesn't translate very well&#8212;English doesn't have a simple description for it, or any other culture I know.  Maybe the passage in the Old Testament, "Thou shalt not boil a young goat in its mother's milk".  Someone who helps you out of altruism shouldn't regret helping you; you owe them, not so much fealty, but rather, that they're actually doing what they think they're doing by helping you.</p> <p>Well, but how would Brian Atkins find out, if I don't tell him?  Eliezer<sub>2000</sub> doesn't even <em>think</em> this except in quotation marks, as the obvious thought that a villain would think in the same situation.  And Eliezer<sub>2000</sub> has a standard counter-thought ready too, a ward against temptations to dishonesty&#8212;an argument that justifies honesty in terms of expected utility, not just a personal love of personal virtue:</p> <blockquote> <p>Human beings aren't perfect deceivers; it's likely that I'll be found out.  Or what if genuine lie detectors are invented before the Singularity, sometime over the next thirty years?  I wouldn't be able to pass a lie detector test.</p> </blockquote> <p>Eliezer<sub>2000</sub> lives by the rule that you should always be ready to have your thoughts broadcast to the whole world at any time, without embarrassment.  Otherwise, clearly, you've fallen from grace: either you're thinking something you shouldn't be thinking, or you're embarrassed by something that shouldn't embarrass you.</p> <p>(These days, I don't espouse quite such an extreme viewpoint, mostly for reasons of Fun Theory.  I see a role for continued social competition between intelligent life-forms, as least as far as my near-term vision stretches.  I admit, these days, that it might be all right for human beings to have a self; as John McCarthy put it, "If everyone were to live for others all the time, life would be like a procession of ants following each other around in a circle."  If you're going to have a self, you may as well have secrets, and maybe even conspiracies.  But I do still try to abide by the principle of being able to pass a future lie detector test, with anyone else who's also willing to go under the lie detector, if the topic is a professional one.  Fun Theory needs a commonsense exception for global catastrophic risk management.)</p> <p>Even taking honesty for granted, there are other excuses Eliezer<sub>2000</sub> could use to flush the question down the toilet.  "The world doesn't have the time" or "It's unsolvable" would still work.  But Eliezer <sub>2000</sub> doesn't <em>know</em> that this problem, the "backup" morality problem, is going to be particularly difficult or time-consuming.  He's just now thought of the whole issue.</p> <p>And so Eliezer<sub>2000</sub> <em>begins</em> to really consider the question:  Supposing that "life is meaningless" (that superintelligences <em>don't</em> produce their own motivations from pure logic), then how would you go about specifying a <em>fallback</em> morality?  Synthesizing it, inscribing it into the AI?</p> <p>There's a lot that Eliezer<sub>2000</sub> doesn't know, at this point.  But he <em>has</em> been thinking about self-improving AI for three years, and he's been a Traditional Rationalist for longer than that.  There are techniques of rationality that he <em>has</em> practiced, methodological safeguards he's already devised.  He already knows better than to think that all an AI needs is the <a href="0184.html">One Great Moral Principle</a> [http://lesswrong.com/lw/lq/fake_utility_functions/].  Eliezer<sub>2000</sub> already knows that it is wiser to think technologically than politically.  He already knows the saying that AI programmers are supposed to think in code, to use concepts that can be inscribed in a computer.  Eliezer<sub>2000</sub> already has a concept that there is something called "technical thinking" and it is good, though he hasn't yet formulated a Bayesian view of it. And he's long since noticed that  <a href="0167.html">suggestively named LISP tokens</a> [http://lesswrong.com/lw/l9/artificial_addition/] don't really mean anything, etcetera.  These injunctions prevent him from falling into some of the initial traps, the ones that I've seen consume other novices on their own first steps into the Friendly AI problem... though technically this was my <em>second</em> step; I well and truly failed on my first.</p> <p>But in the end, what it comes down to is this:  For the first time, Eliezer<sub>2000</sub> is trying to think technically about inscribing a morality into an AI, without the escape-hatch of the mysterious essence of rightness.</p> <p>That's the only thing that matters, in the end.  His previous philosophizing wasn't enough to force his brain to confront the details.  This new standard is strict enough to require actual work.  Morality slowly starts being less mysterious to him&#8212;Eliezer<sub>2000</sub> is starting to think <em>inside</em> the black box.</p> <p>His <em>reasons</em> for pursuing this course of action&#8212;those don't matter at all.</p> <p>Oh, there's a lesson in his being a perfectionist.  There's a lesson in the part about how Eliezer<sub>2000</sub> initially thought this was a tiny flaw, and could have dismissed it out-of-mind if that had been his impulse.</p> <p>But in the end, the chain of cause and effect goes like this:  Eliezer<sub>2000</sub> investigated in more detail, therefore he got better with practice.  Actions screen off justifications.  If your arguments happen to justify not working things out in detail, like Eliezer<sub>1996</sub>, then you won't get good at thinking about the problem.  If your arguments call for you to work things out in detail, then you have an <em>opportunity</em> to start accumulating expertise.</p> <p>That was the only choice that mattered, in the end&#8212;not the <em>reasons </em>for doing anything.</p> <p>I say all this, as you may well guess, because of the AI wannabes I sometimes run into, who have their own clever reasons for not thinking about the Friendly AI problem.  Our clever reasons for doing what we do, tend to matter a lot less to Nature than they do to ourselves and our friends.  If your actions don't look good when they're stripped of all their justifications and presented as mere brute facts... then maybe you should re-examine them.</p> <p>A diligent effort won't always save a person.  There is such a thing as lack of ability.  Even so, if you don't try, or don't try hard enough, you don't get a chance to sit down at the high-stakes table&#8212;never mind the ability ante.  That's cause and effect for you.</p> <p>Also, perfectionism really matters.  The end of the world doesn't always come with trumpets and thunder and the highest priority in your inbox.  Sometimes the shattering truth first presents itself to you as a small, small question; a single discordant note; one tiny lonely thought, that you could dismiss with one easy effortless touch...</p> <p>...and so, over succeeding years, understanding begins to dawn on that past Eliezer, slowly.  That sun rose slower than it could have risen.  To be continued.</p> <p> </p> <p style="text-align:right">Part of the sequence <a href="http://wiki.lesswrong.com/wiki/Yudkowsky%27s_coming_of_age"><em>Yudkowsky's Coming of Age</em></a> [http://wiki.lesswrong.com/wiki/Yudkowsky%27s_coming_of_age]</p> <p style="text-align:right">Next post: "<a href="0490.html">Fighting a Rearguard Action Against the Truth</a> [http://lesswrong.com/lw/u8/fighting_a_rearguard_action_against_the_truth/]"</p> <p style="text-align:right">Previous post: "<a href="0484.html">The Sheer Folly of Callow Youth</a> [http://lesswrong.com/lw/u2/the_sheer_folly_of_callow_youth/]"</p></div> <hr><table><tr><th colspan="2"><a href="seq22.html">Sequence 22: Yudkowsky's Coming of Age</a>:</th></tr><tr><td><p><i>Previous: </i><a href="0484.html">The Sheer Folly of Callow Youth</a></p></td><td><p><i>Next: </i><a href="0490.html">Fighting a Rearguard Action Against the Truth</a></p></td></tr></table><p><i>Referenced by: </i><a href="0484.html">The Sheer Folly of Callow Youth</a> &#8226; <a href="0490.html">Fighting a Rearguard Action Against the Truth</a> &#8226; <a href="0503.html">My Bayesian Enlightenment</a> &#8226; <a href="0517.html">Protected From Myself</a></p><p><i>Original with comments: </i><a href="http://lesswrong.com/lw/u7/that_tiny_note_of_discord/">That Tiny Note of Discord</a></p></body></html>